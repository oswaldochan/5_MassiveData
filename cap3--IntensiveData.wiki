__TOC__

= Chapter 3 =

=== What is a ''log'' according to the book? ===

an append-only sequence of records. It doesn’t have to be human-readable; it might be binary and intended only for other programs to read.

=== What is an index according to the book? ===

An index is an additional structure that is derived from the primary data. Many databases allow you to add and remove indexes, and this doesn’t affect the contents of the database; it only affects the performance of queries. Maintaining additional structures incurs overhead, especially on writes. For writes, it’s hard to beat the performance of simply appending to a file, because that’s the simplest possible write operation. Any kind of index usually slows down writes, because the index also needs to be updated every time data is written.<br />
This is an important trade-off in storage systems: well-chosen indexes speed up read<br />
queries, but every index slows down writes.

== Data Structures That Power Your Database ==

=== How does hash indexes work? ===

Let’s say our data storage consists only of appending to a file, as in the preceding example. Then the simplest possible indexing strategy is this: keep an in-memory hash map where every key is mapped to a byte offset in the data file—the location at which the value can be found. Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote (this works both for inserting new keys and for updating existing keys). When you want to look up a value, use the hash map to find the offset in the data file, seek to that location, and read the value

=== What are some issues of real implentation? ===

* File format: CSV is not the best format for a log. It’s faster and simpler to use a binary format
* Deleting records: If you want to delete a key and its associated value, you have to append a special deletion record to the data file (sometimes called a tombstone)
* Crash recovery: If the database is restarted, the in-memory hash maps are lost.
* Partially written records: The database may crash at any time, including halfway through appending a record to the log
* Concurrency control: As writes are appended to the log in a strictly sequential order, a common implementation choice is to have only one writer thread. Data file segments are append-only and otherwise immutable, so they can be read concurrently by multiple threads.

=== Why an append-only design can be a good idea? ===

* Appending and segment merging are sequential write operations, which are generally much faster than random writes, especially on magnetic spinning-disk hard drives.
* Concurrency and crash recovery are much simpler if segment files are append-only or immutable.
* Merging old segments avoids the problem of data files getting fragmented over time.

=== What are SSTables? ===

Sorted String Table, and is a format for sequences of key-value sorted by key

=== What are some advantages of SSTables? ===

* Merging segments is simple and efficient, even if the files are bigger than the available memory. 
*  In order to find a particular key in the file, you no longer need to keep an index of all the keys in memory.
*  Since read requests need to scan over several key-value pairs in the requested range anyway, it is possible to group those records into a block and compress it before writing it to disk

=== What are B-Trees? ===

It is the most widely used indexing structure.

Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key-value lookups and range queries. But that’s where the similarity ends: B-trees have a very different design philosophy.<br />
B-trees break the database down into fixed-size blocks or pages, traditionally 4 KB in size (sometimes bigger), and read or write one page at a time. This design corresponds more closely to the underlying hardware, as disks are also arranged in fixed-size blocks. 

Each page can be identified using an address or location, which allows one page to refer to another—similar to a pointer, but on disk instead of in memory. We can use these page references to construct a tree of pages

=== What are some ways B-Trees had been optimized? ===

*  Instead of overwriting pages and maintaining a WAL for crash recovery, some databases (like LMDB) use a copy-on-write scheme
*  We can save space in pages by not storing the entire key, but abbreviating it.
*  In general, pages can be positioned anywhere on disk; there is nothing requiring pages with nearby key ranges to be nearby on disk.
* Additional pointers have been added to the tree
* B-tree variants such as fractal trees borrow some log-structured ideas to reduce disk seeks (and they have nothing to do with fractals).

=== What is the advantage of LSM-Trees ''(Log-Structured Merge-Tree)''? ===

SM-trees are typically able to sustain higher write throughput than B-trees, partly because they sometimes have lower write amplification (although this depends on the storage engine configuration and workload), and partly because they sequentially write compact SSTable files rather than having to overwrite several pages in the tree. This difference is particularly important on magnetic hard drives,where sequential writes are much faster than random writes.<br />
LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees. B-tree storage engines leave some disk space unused due to fragmentation: when a page is split or when a row cannot fit into an existing page, some space in a page remains unused. Since LSM-trees are not page-oriented and periodically rewrite SSTables to remove fragmentation, they have lower storage overheads, especially when using leveled compaction.

=== What is a disadvantage of LSM-Trees? ===

A downside of log-structured storage is that the compaction process can sometimes interfere with the performance of ongoing reads and writes. Even though storage engines try to perform compaction incrementally and without affecting concurrent access, disks have limited resources, so it can easily happen that a request needs to wait while the disk finishes an expensive compaction operation.

=== Name other indexing structures: ===

* Storing values within the index
* Multi-column indexes
* Full-text search and fuzzy search

== Transaction Processing or Analytics ==

=== What are some properties of ''transactions''? ===

A transaction needn’t necessarily have ACID (atomicity, consistency, isolation, and durability) properties. Transaction processing just means allowing clients to make low-latency reads and writes—as opposed to batch processing jobs, which only run periodically (for example, once per day).

=== What is online transaction processing (OLTP? ===

When an application typically looks up a small number of records by some key, using an index. Records<br />
are inserted or updated based on the user’s input. Because these applications are interactive, the access pattern became known as online transaction processing

=== What is a data warehouse? ===

A data warehouse, by contrast, is a separate database that analysts can query to their hearts’ content, without affecting OLTP operations [48]. The data warehouse contains a read-only copy of the data in all the various OLTP systems in the company.<br />
Data is extracted from OLTP databases (using either a periodic data dump or a continuous stream of updates), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse. This process of getting data into the warehouse is known as Extract–Transform–Load (ETL).

Data warehouses now exist in almost all large enterprises, but in small companies they are almost unheard of.

=== What is ''star-schema''? ===

The name “star schema” comes from the fact that when the table relationships are visualized, the fact table is in the middle, surrounded by its dimension tables; the connections to these tables are like the rays of a star.

=== What is ''snowflake-schema''? ===

A variation of the star schema is known as the snowflake schema, where dimensions are<br />
further broken down into subdimensions. 

Snowflake schemas are more normalized than star schemas, but star schemas are often preferred because they are simpler for analysts to work<br />
with.

== Column-Oriented Storage ==

=== What is Column-Oriented storage? ===

The idea behind column-oriented storage is simple: don’t store all the values from one row together, but store all the values from each column together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in that query, which can save a lot of work.

Column storage is easiest to understand in a relational data model, but it applies equally to nonrelational data. For example, Parquet is a columnar storage format that supports a document data model, based on Google’s Dremel.

=== What is bitmap encoding? ===

Often, the number of distinct values in a column is small compared to the number of rows (for example, a retailer may have billions of sales transactions, but only 100,000 distinct products). We can now take a column with n distinct values and turn it into n separate bitmaps: one bitmap for each distinct value, with one bit for each row. The bit is 1 if the row has that value, and 0 if not.<br />
If n is very small (for example, a country column may have approximately 200 distinct values), those bitmaps can be stored with one bit per row. But if n is bigger, there will be a lot of zeros in most of the bitmaps (we say that they are sparse). In that case, the bitmaps can additionally be run-length encoded. This can make the encoding of a column remarkably compact.

=== How can Column-Oriented Storage take advantage of the CPU? ===

Besides reducing the volume of data that needs to be loaded from disk, column-oriented storage layouts are also good for making efficient use of CPU cycles. For example, the query engine can take a chunk of compressed column data that fits comfortably in the CPU’s L1 cache and iterate through it in a tight loop (that is, with no function calls). A CPU can execute such a loop much faster than code that requires a lot of function calls and conditions for each record that is processed. Column compression allows more rows from a column to fit in the same amount of L1 cache. Operators, such as the bitwise AND and OR described previously, can be designed to operate on such chunks of compressed column data directly. This technique is known as '''vectorized processing'''.

=== How does sort order works in Column Storage? ===

the data needs to be sorted an entire row at a time, even though it is stored by column. For example, if queries often target date ranges, such as the last month, it might make sense to make date_key the first sort key. Then the query optimizer can scan only the rows from the last month, which will be much faster than scanning all rows.<br />
A second column can determine the sort order of any rows that have the same value in the first column. That will help queries that need to group or filter sales by product within a certain date range.<br />
Another advantage of sorted order is that it can help with compression of columns. If the primary sort column does not have many distinct values, then after sorting, it will have long sequences where the same value is repeated many times in a row

=== How is the writing process in Column-Oriented Storage? ===

All writes first go to an in-memory store, where they are added to a sorted structure and prepared for writing to disk. It doesn’t matter whether the in-memory store is row-oriented or column-oriented. When enough writes have accumulated, they are merged with the column files on disk and written to new files in bulk
